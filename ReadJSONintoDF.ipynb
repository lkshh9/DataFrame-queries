{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d491a5a8-ed6a-4a22-8cc9-2b98962bf603",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrameReader in module pyspark.sql.readwriter object:\n\nclass DataFrameReader(OptionUtils)\n |  DataFrameReader(spark: 'SparkSession')\n |  \n |  Interface used to load a :class:`DataFrame` from external storage systems\n |  (e.g. file systems, key-value stores, etc). Use :attr:`SparkSession.read`\n |  to access this.\n |  \n |  .. versionadded:: 1.4.0\n |  \n |  .. versionchanged:: 3.4.0\n |      Support Spark Connect.\n |  \n |  Method resolution order:\n |      DataFrameReader\n |      OptionUtils\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, spark: 'SparkSession')\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  csv(self, path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[int, str, NoneType] = None, maxCharsPerColumn: Union[int, str, NoneType] = None, maxMalformedLogPerPartition: Union[int, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame'\n |      Loads a CSV file and returns the result as a  :class:`DataFrame`.\n |      \n |      This function will go through the input once to determine the input schema if\n |      ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n |      ``inferSchema`` option or specify the schema explicitly using ``schema``.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str or list\n |          string, or list of strings, for input path(s),\n |          or RDD of Strings storing CSV rows.\n |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n |          an optional :class:`pyspark.sql.types.StructType` for the input schema\n |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a CSV file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a CSV file\n |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n |      ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n |      ...\n |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n |      ...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()\n |      +---+----+\n |      |age|name|\n |      +---+----+\n |      |100|null|\n |      +---+----+\n |  \n |  format(self, source: str) -> 'DataFrameReader'\n |      Specifies the input data source format.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      source : str\n |          string, name of the data source, e.g. 'json', 'parquet'.\n |      \n |      Examples\n |      --------\n |      >>> spark.read.format('json')\n |      <...readwriter.DataFrameReader object ...>\n |      \n |      Write a DataFrame into a JSON file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a JSON file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n |      ...\n |      ...     # Read the JSON file as a DataFrame.\n |      ...     spark.read.format('json').load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  jdbc(self, url: str, table: str, column: Optional[str] = None, lowerBound: Union[int, str, NoneType] = None, upperBound: Union[int, str, NoneType] = None, numPartitions: Optional[int] = None, predicates: Optional[List[str]] = None, properties: Optional[Dict[str, str]] = None) -> 'DataFrame'\n |      Construct a :class:`DataFrame` representing the database table named ``table``\n |      accessible via JDBC URL ``url`` and connection ``properties``.\n |      \n |      Partitions of the table will be retrieved in parallel if either ``column`` or\n |      ``predicates`` is specified. ``lowerBound``, ``upperBound`` and ``numPartitions``\n |      is needed when ``column`` is specified.\n |      \n |      If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      table : str\n |          the name of the table\n |      column : str, optional\n |          alias of ``partitionColumn`` option. Refer to ``partitionColumn`` in\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n |          for the version you use.\n |      predicates : list, optional\n |          a list of expressions suitable for inclusion in WHERE clauses;\n |          each one defines one partition of the :class:`DataFrame`\n |      properties : dict, optional\n |          a dictionary of JDBC database connection arguments. Normally at\n |          least properties \"user\" and \"password\" with their corresponding values.\n |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Notes\n |      -----\n |      Don't create too many partitions in parallel on a large cluster;\n |      otherwise Spark might crash your external database systems.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |  \n |  json(self, path: Union[str, List[str], pyspark.rdd.RDD[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, primitivesAsString: Union[bool, str, NoneType] = None, prefersDecimal: Union[bool, str, NoneType] = None, allowComments: Union[bool, str, NoneType] = None, allowUnquotedFieldNames: Union[bool, str, NoneType] = None, allowSingleQuotes: Union[bool, str, NoneType] = None, allowNumericLeadingZero: Union[bool, str, NoneType] = None, allowBackslashEscapingAnyCharacter: Union[bool, str, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, allowUnquotedControlChars: Union[bool, str, NoneType] = None, lineSep: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, dropFieldIfAllNull: Union[bool, str, NoneType] = None, encoding: Optional[str] = None, locale: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, allowNonNumericNumbers: Union[bool, str, NoneType] = None) -> 'DataFrame'\n |      Loads JSON files and returns the results as a :class:`DataFrame`.\n |      \n |      `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n |      For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n |      \n |      If the ``schema`` parameter is not specified, this function goes\n |      through the input once to determine the input schema.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str, list or :class:`RDD`\n |          string represents path to the JSON dataset, or a list of paths,\n |          or RDD of Strings storing JSON objects.\n |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n |          an optional :class:`pyspark.sql.types.StructType` for the input schema or\n |          a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a JSON file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a JSON file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n |      ...\n |      ...     # Read the JSON file as a DataFrame.\n |      ...     spark.read.json(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  load(self, path: Union[str, List[str], NoneType] = None, format: Optional[str] = None, schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n |      Loads data from a data source and returns it as a :class:`DataFrame`.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str or list, optional\n |          optional string or a list of string for file-system backed data sources.\n |      format : str, optional\n |          optional string for format of the data source. Default to 'parquet'.\n |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n |          optional :class:`pyspark.sql.types.StructType` for the input schema\n |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n |      **options : dict\n |          all other string options\n |      \n |      Examples\n |      --------\n |      Load a CSV file with format, schema and options specified.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a CSV file with a header\n |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n |      ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n |      ...\n |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n |      ...     # and 'header' option set to `True`.\n |      ...     df = spark.read.load(\n |      ...         d, schema=df.schema, format=\"csv\", nullValue=\"Hyukjin Kwon\", header=True)\n |      ...     df.printSchema()\n |      ...     df.show()\n |      root\n |       |-- age: long (nullable = true)\n |       |-- name: string (nullable = true)\n |      +---+----+\n |      |age|name|\n |      +---+----+\n |      |100|null|\n |      +---+----+\n |  \n |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameReader'\n |      Adds an input option for the underlying data source.\n |      \n |      .. versionadded:: 1.5.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      key : str\n |          The key for the option to set.\n |      value\n |          The value for the option to set.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql import SparkSession\n |      >>> spark = SparkSession.builder.master(\"local\").getOrCreate()\n |      >>> spark.read.option(\"key\", \"value\")\n |      <...readwriter.DataFrameReader object ...>\n |      \n |      Specify the option 'nullValue' with reading a CSV file.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a CSV file\n |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n |      ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n |      ...\n |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n |      ...     spark.read.schema(df.schema).option(\n |      ...         \"nullValue\", \"Hyukjin Kwon\").format('csv').load(d).show()\n |      +---+----+\n |      |age|name|\n |      +---+----+\n |      |100|null|\n |      +---+----+\n |  \n |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameReader'\n |      Adds input options for the underlying data source.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      **options : dict\n |          The dictionary of string keys and prmitive-type values.\n |      \n |      Examples\n |      --------\n |      >>> spark.read.option(\"key\", \"value\")\n |      <...readwriter.DataFrameReader object ...>\n |      \n |      Specify the option 'nullValue' and 'header' with reading a CSV file.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a CSV file with a header.\n |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n |      ...     df.write.option(\"header\", True).mode(\"overwrite\").format(\"csv\").save(d)\n |      ...\n |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon',\n |      ...     # and 'header' option set to `True`.\n |      ...     spark.read.options(\n |      ...         nullValue=\"Hyukjin Kwon\",\n |      ...         header=True\n |      ...     ).format('csv').load(d).show()\n |      +---+----+\n |      |age|name|\n |      +---+----+\n |      |100|null|\n |      +---+----+\n |  \n |  orc(self, path: Union[str, List[str]], mergeSchema: Optional[bool] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n |      Loads ORC files, returning the result as a :class:`DataFrame`.\n |      \n |      .. versionadded:: 1.5.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str or list\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a ORC file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a ORC file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.mode(\"overwrite\").format(\"orc\").save(d)\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.orc(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  parquet(self, *paths: str, **options: 'OptionalPrimitiveType') -> 'DataFrame'\n |      Loads Parquet files, returning the result as a :class:`DataFrame`.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      paths : str\n |      \n |      Other Parameters\n |      ----------------\n |      **options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a Parquet file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.parquet(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  schema(self, schema: Union[pyspark.sql.types.StructType, str]) -> 'DataFrameReader'\n |      Specifies the input schema.\n |      \n |      Some data sources (e.g. JSON) can infer the input schema automatically from data.\n |      By specifying the schema here, the underlying data source can skip the schema\n |      inference step, and thus speed up data loading.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      schema : :class:`pyspark.sql.types.StructType` or str\n |          a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\n |          (For example ``col0 INT, col1 DOUBLE``).\n |      \n |      Examples\n |      --------\n |      >>> spark.read.schema(\"col0 INT, col1 DOUBLE\")\n |      <...readwriter.DataFrameReader object ...>\n |      \n |      Specify the schema with reading a CSV file.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     spark.read.schema(\"col0 INT, col1 DOUBLE\").format(\"csv\").load(d).printSchema()\n |      root\n |       |-- col0: integer (nullable = true)\n |       |-- col1: double (nullable = true)\n |  \n |  table(self, tableName: str) -> 'DataFrame'\n |      Returns the specified table as a :class:`DataFrame`.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      tableName : str\n |          string, name of the table.\n |      \n |      Examples\n |      --------\n |      >>> df = spark.range(10)\n |      >>> df.createOrReplaceTempView('tblA')\n |      >>> spark.read.table('tblA').show()\n |      +---+\n |      | id|\n |      +---+\n |      |  0|\n |      |  1|\n |      |  2|\n |      |  3|\n |      |  4|\n |      |  5|\n |      |  6|\n |      |  7|\n |      |  8|\n |      |  9|\n |      +---+\n |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n |  \n |  text(self, paths: Union[str, List[str]], wholetext: bool = False, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None) -> 'DataFrame'\n |      Loads text files and returns a :class:`DataFrame` whose schema starts with a\n |      string column named \"value\", and followed by partitioned columns if there\n |      are any.\n |      The text files must be encoded as UTF-8.\n |      \n |      By default, each line in the text file is a new row in the resulting DataFrame.\n |      \n |      .. versionadded:: 1.6.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      paths : str or list\n |          string, or list of strings, for input path(s).\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a text file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a text file\n |      ...     df = spark.createDataFrame([(\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n |      ...     df.write.mode(\"overwrite\").format(\"text\").save(d)\n |      ...\n |      ...     # Read the text file as a DataFrame.\n |      ...     spark.read.schema(df.schema).text(d).sort(\"alphabets\").show()\n |      +---------+\n |      |alphabets|\n |      +---------+\n |      |        a|\n |      |        b|\n |      |        c|\n |      +---------+\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from OptionUtils:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n"
     ]
    }
   ],
   "source": [
    "help(spark.read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "539b6619-19a7-432b-94a2-ff875c05654d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- gender: string (nullable = true)\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: string (nullable = true)\n\n+------+---+--------+------+\n|gender| id|    name|salary|\n+------+---+--------+------+\n|  male|  1|John Doe|  2000|\n|  male|  2|    Oggy| 20000|\n+------+---+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(path='dbfs:/FileStore/data/emps.json')\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1951e62-e834-4aac-8a7c-2429296206f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- gender: string (nullable = true)\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+---+--------+------+\n|gender| id|    name|salary|\n+------+---+--------+------+\n|  male|  1|John Doe|  2000|\n|  male|  2|    Oggy| 20000|\n+------+---+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.json(path='dbfs:/FileStore/data/emps1.json')\n",
    "df1.printSchema()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dabfb0b-8434-44ef-a50f-2769886b22c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _corrupt_record: string (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2916291915317104>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m df2 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mjson(path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdbfs:/FileStore/data/emps2.json\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m      2\u001B[0m df2\u001B[38;5;241m.\u001B[39mprintSchema()\n",
       "\u001B[0;32m----> 3\u001B[0m df2\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n",
       "\u001B[1;32m    914\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m    915\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_BOOLEAN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    916\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m    917\u001B[0m     )\n",
       "\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n",
       "\u001B[0;32m--> 920\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m    921\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    922\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\n",
       "referenced columns only include the internal corrupt record column\n",
       "(named _corrupt_record by default). For example:\n",
       "spark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\n",
       "and spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\n",
       "Instead, you can cache or save the parsed results and then send the same query.\n",
       "For example, val df = spark.read.schema(schema).csv(file).cache() and then\n",
       "df.filter($\"_corrupt_record\".isNotNull).count()."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2916291915317104>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m df2 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mjson(path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdbfs:/FileStore/data/emps2.json\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m df2\u001B[38;5;241m.\u001B[39mprintSchema()\n\u001B[0;32m----> 3\u001B[0m df2\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:920\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    914\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m    915\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_BOOLEAN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    916\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m    917\u001B[0m     )\n\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n\u001B[0;32m--> 920\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    921\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    922\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2 = spark.read.json(path='dbfs:/FileStore/data/emps2.json')\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea165242-5e25-4433-b5b7-276c6b98695a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read Multiline JSON File ⬇️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce9b6ea3-6563-4ee9-a678-6b28abc09987",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- gender: string (nullable = true)\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+---+-----------+------+\n|gender| id|       name|salary|\n+------+---+-----------+------+\n|  male|  1|Katana bhai|200000|\n|  male|  2|       Oggy|  6000|\n|  male|  3|David Putra|  8000|\n|  male|  4|   Nobidora| 12000|\n+------+---+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.json(path='dbfs:/FileStore/data/emps2.json',multiLine=True)\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72a0dba7-60cd-49f7-932c-c59f08055e22",
     "showTitle": true,
     "title": ""
    }
   },
   "source": [
    "## Read Multiple JSON Files ⬇️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf871f75-a850-458e-9dfa-3cdd341209a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------+------+\n|gender| id|    name|salary|\n+------+---+--------+------+\n|  male|  1|John Doe|  2000|\n|  male|  2|    Oggy| 20000|\n|  male|  1|John Doe|  2000|\n|  male|  2|    Oggy| 20000|\n+------+---+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.json(['dbfs:/FileStore/data/emps.json','dbfs:/FileStore/data/emps1.json'])\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb9822b-7293-4d63-acea-d7bc9c9caf52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- gender: string (nullable = true)\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+---+--------+------+\n|gender| id|    name|salary|\n+------+---+--------+------+\n|  male|  1|John Doe|  2000|\n|  male|  2|    Oggy| 20000|\n|  male|  3|   guddu| 45000|\n|  male|  4|    golu| 50000|\n+------+---+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.json(path=['dbfs:/FileStore/data/emps1.json','dbfs:/FileStore/data/emps3.json'])\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da466a1a-1cfd-4f87-a0a2-b85d39a5053d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ed3e52a-f6e9-437e-807f-c7f44abba894",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Enforcing Schema on top of dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "815a1b42-d8ec-4cdb-8482-582c01eed279",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---+--------+------+------+\n| id|    name|gender|salary|\n+---+--------+------+------+\n|  1|John Doe|  male|  2000|\n|  2|    Oggy|  male| 20000|\n|  3|   guddu|  male| 45000|\n|  4|    golu|  male| 50000|\n+---+--------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "schema = StructType().add(field=\"id\",data_type=IntegerType())\\\n",
    "            .add(field=\"name\",data_type=StringType())\\\n",
    "            .add(field=\"gender\",data_type=StringType())\\\n",
    "            .add(field=\"salary\",data_type=IntegerType())\n",
    "\n",
    "df3 = spark.read.json(path=['dbfs:/FileStore/data/emps1.json','dbfs:/FileStore/data/emps3.json'], schema=schema)\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ReadJSONintoDF",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
